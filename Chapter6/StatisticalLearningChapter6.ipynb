{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Model Selection and Regularization\n",
    "\n",
    "There are some ways through which a simple linear model can be improved by replacing the least square fitting with some alternative fitting procedures. Why would one want to do that you might ask? Well that is because alternative fitting procedures can yield better prediction accuracy and model interpretability.\n",
    "\n",
    "1. <strong> Model Accuracy: </strong> Provided that the true relationship between the response and the predictors is approximately linear, the least squares method will have low bias. If n >> p, then the least squares estimates tend to also have low variance and hence will obviously perform better on the test set. However, if n is not much larger than p then there will be a lot of variability in the least squares fit resulting in overfitting and consequently poor prediction  on future observations not used in the model training. And if p > n, then there is no longer a unique least squares estimate: the variance is infinite so the method cannot be used at all. By constraining or shrinking the estimated coefficients, we can substantially reduce the variance at the cost of a negligible increase in bias. This can lead to substantial improvements in the accuracy.\n",
    "\n",
    "2. <strong> Model Interpretability: </strong> It is often the case thaat some or many of the variables used in multiple regression model are in fact not associated with the response variable. Including such irrelevant variables leads to unnecessary complexity in the resulting model. By removing these variables - we can obtain a model that is easily interpreted. Now least squares is extremely unlikely to yield any coefficient estimates that are exactly 0 (basically removing them).\n",
    "\n",
    "There are many alterntatives, both classical and modern, to using least squares to fit. Some of the important ones are:\n",
    "\n",
    "## Subset Selection\n",
    "\n",
    "This approach involves a subset of the p predictors that we believe to be related to the response. We then fit a model using least squares on the reduced set of variables.\n",
    "\n",
    "## Shrinkage\n",
    "\n",
    "This method involves fitting a model involving all the p predictors. However, the estimated coefficients are shrunken towards zero relative to the least squares estimates. This Shrinkage (also called regularization) has the effect of reducing variance. Depending on what type of shrinkage is performed, some of the estimates may be estimated to be exactly zero. Hence shrinkage methods can also perform variable selection.\n",
    "\n",
    "## Dimension Reduction\n",
    "\n",
    "This method involves projecting the p predictors into an M dimension subspace where M < p. This is achieved by computing M different linear combinations or projections of the variables. Then these M projections are used as predictors to fit a linear regression model by the least squares methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subset Selection\n",
    "There are quite a few methods for selecting subsets of predictors. These include best subset and stepwise model selection procedures\n",
    "## Best Subset\n",
    "To perform best subset, we fit a seperate least squares regression for each possible combination of the p predictors. That is, we fit all p models that contain exactly one predictor, all PC2 models that contain exactly two predictors and so on. We then look at the resulting models, with the goal of identifying the one that is best.\n",
    "\n",
    "<strong> Algorithm: </strong> \n",
    "1. Let M_0 denote the null model which contains no predictors. This model simply calculates the sample mean for each observation.\n",
    "2. For K = 1,2,3 ... P\n",
    "    (a) Fit all PCk models that contain exactly k predictors.\n",
    "    (b) Pick the best among the PCK models and call it M_K. Here the best is defined as the         smallest RSS.\n",
    "3. Select a single best model among M_0, M_1,. ...M_P using cross-validated prediction          error, C_p, BIC or adjusted R^2.\n",
    "Why do we use these methods in step 3 instead of RSS? The problem is that a low RSS indicates a model with low training data whereas we wish to choose a model that has a low test error.\n",
    "\n",
    "Although the algorithm mentioned above uses least square regression, the same ideas apply to other types of models such as logistic regression. In the case of logistic regression, instead of ordering models by RSS in step 2, we instead use deviance, a measure that plays the role of RSS for a broader class of models. The deviance is the negative times two the maximized likelihood. The smaller the deviance the better.\n",
    "While best subset selection is a simple and conceptually appealing approach, it suffers from computational limitations. The number of possible models that must be considered grows rapidly as p increases. Consequently, the best subset selection becomes computationally infeasible for values of p greater than around 40, even with fast modern supercomputers.\n",
    "\n",
    "# Stepwise Selection\n",
    "Best subset may also suffere from statistical problems when p is large. The larger the subspace, the higher the chances of finding models that look good on the training data, even though they may not have any predictive power on future data. Thus an enormous search space can lead to overfitting and high variance of the coefficient estimates. For both these reasons, stepwise methods, which explore a far more restrictive set of models are attractive alternatives to best subset selection.\n",
    "\n",
    "## Forward Stepwise Selection\n",
    "Forward stepwise selection begins with a model containing no predictors and then adds predictors to the model, one at a time, untill all the predictors are in the model. In particular, at each sstep, the variable that gives the greatest additional improvement to the fit is added to the model.\n",
    "\n",
    "<strong> Algorithm: </strong> \n",
    "1. Let M_0 denote the null model which contains no predictors. \n",
    "2. For K = 1,2,3 ... P - 1\n",
    "    (a) Consider all p - k models that augment the predictors i M_K with one additional             predictor.\n",
    "    (b) Choose the best among the p - k models and call it M_(K+1). Here best fit is defined         as the one with the smallest training RSS.\n",
    "3. Select a single best model among M_0, M_1,. ...M_P using cross-validated prediction          error, C_p, BIC or adjusted R^2.\n",
    "\n",
    "Unlike best subset selection which involved fitting 2^p models, forward stepwise selection involves fitting 1 + p(p+1)/2 models which is a significant difference. Although this method tends to do well in practise, it is not guarranteed to find the best possible model out of all the 2^p models.\n",
    "\n",
    "## Backward Stepwise Selection\n",
    "Unlike forward stepwise selection, it begins with the full least squares model containing all p predictors and then iteratively removes the least useful predictor, one-at-a-time,\n",
    "\n",
    "<strong> Algorithm: </strong> \n",
    "1. Let M_p denote the full model which contains all the p predictors. \n",
    "2. For K = p, p-1 ..... 1\n",
    "    (a) Consider all k models that contain all but one of the predictors in M_k, for a total         of k - 1 predictors.\n",
    "    (b) Choose the best among the k models and call it M_(K+1). Here best fit is defined             as the one with the smallest training RSS.\n",
    "3. Select a single best model among M_0, M_1,. ...M_P using cross-validated prediction          error, C_p, BIC or adjusted R^2.\n",
    "\n",
    "Backward stepwise selection requires that the number of samples n is larger than p. In contrast forward stepwise selection can be used even when n < p and so is the only viable subset method when p is very large.\n",
    "\n",
    "## Hybrid approaches\n",
    "Variables are added to the model sequentially in analogy to forward selection. However, after adding each new variable, the method may also remove any variables that no longer provide an improvement in the model fit.\n",
    "\n",
    "# Choosing the Optimal Model\n",
    "In order to implement the above mentioned methods,we need a way to determine which of the models is the best. Training error can be a poor estimate of the test error as we all know. Therefor R^2 and RSS are not suitable for selecting the best model among a collection of models with different number of predictors. In order to select the best model with respect to test error, we need to estimate this test error. There are two common approaches:\n",
    "\n",
    "1. We can indirectly estimate the test error by making adjustment to the training error to      account for bias due to overfitting.\n",
    "2. We can directly estimate the test error using either a validation set approach or a          cross validation approach.\n",
    "\n",
    "## AIC, BIC, Adjusted R Squared\n",
    "\n",
    "We know that the training set MSE is generally an understimate of the test MSE. However, a number of techniques for adjusting the training error for the model size are available. These approaches can be used among a set of models with different number of variables. There are 4 such approaches: <strong> Cp, Akaike information criterion (AIC), Bayesion information criterior (BIC) and Adjusted R squared </strong>.\n",
    "\n",
    "For a fitted least squares model containig d predictors, the Cp estimate of the test MSE is computed the equation:\n",
    "Cp = (RSS + 2dsigma^2) / n where sigma^2 is the estimate of the variance of the error E associated with each response measurement. Essentially, Cp statistic adds a penalty of 2dsigma^2 to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error. Clearly, the penalty increases with increase in number of predictors. This is intended to adjust for the corresponding decrease in training RSS.\n",
    "\n",
    "The AIC criterion is defined for a large class of models fit by maximum likelihood. \n",
    "AIC = (RSS + 2dsigma^2) / (n * sigma^2). For least square models, Cp and AIC are proportional to each other.\n",
    "\n",
    "BIC is derived from a Bayesian point of view but ends up looking similar to Cp.\n",
    "BIC = (RSS + log(n)dsigma^2) / (n)\n",
    "BIC just places a heavier penalty on models with many variables and hence results in the selection of smaller models than Cp.\n",
    "\n",
    "Adjusted R squared = 1 - RSS/(n-d-1)/TSS(n-1). Unlike Cp, AIC, BIC for which a small value indicates a model with a low test error, a large value of the adjusted R squared indicates a model with small test error.\n",
    "\n",
    "## Validation & Cross-Validation\n",
    "\n",
    "We can compute the validation set error or the cross-validation set error for each model under consideration and then select the model for which the resulting estimated test error is the smallest. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shrinkage Method\n",
    "As an alternative, we can fit a model m containing all p predictors using a technique that constrains or regularizes the coefficient estimates or equivalently that shrinks the coefficient estimates towards zero. It may not be obvious why such a constraint should improve the fit, but it turns out that shrinking the coefficient estimates can significantly reduce their variance. The two best methods for shrinking the regression coefficients towards zero are ridge regression and the lasso.\n",
    "\n",
    "## Ridge Regression\n",
    "Least Square fitting procedure estimates ß0,ß1...ßp using the values that minimize the RSS which is basically ∑(y_i - ß0 - ∑ßjx_ij)^2. \n",
    "Ridge regression is very similar to least squares except that the coefficients are estimated by minimizing a slightly different quantity. In particular, the rifge regression coefficient estimates ß^R are the values that minimize:\n",
    "\n",
    "  <strong>∑(y_i - ß0 - ∑ßjx_ij)^2 + lambda *∑ßj^2 = RSS + lambda *∑ßj^2</strong>\n",
    "\n",
    "where lambda ≥ 0 is a tuning error to be determined seperately. As with least squares. ridge regression seeks coefficients estimates that fit the data well by making the RSS small. However the second term lambda * ∑ßj^2 called \"shrinkage penalty\" is small when ß0,ß1...ßp are close to zero and so has the effect of shrinking the estimates of ß_j towards zero. The tuning parameter lambda serves to control the relative impact of these two terms on the regression coefficient estimates. however as lambda approaches infinity, the imapct of the shrinkage penalty grows and the ridge regression coefficients will approach zero. It will produce different sets of coefficient estimates for each value of lambda, hence selecting a good value of lambda is critical.*It is also a good practise to apply ridge regression after standardization so that everything is on the same scale*.\n",
    "\n",
    "<strong> How does Ridge regression impove over least squares? </strong>\n",
    "\n",
    "The answer lies in the bias - variance tradeoff. In general, in situations, where the relationship between the response and the predictors is close to linear, the leas squares estimates will have low bias but may have high variance. This means that a small change in the training data can cause a large change in the least square coefficients. In particular, when the number of variables p is almost as larg as the number of observations n, the least squares estimate will be extremeley variable. And if p > n, then the least squares estimates do not even have a unique solution, whereas the ridge regression can still perform well by trading off  a small increase in bias for a large decrease in variance.\n",
    "\n",
    "## The Lasso\n",
    "\n",
    "Ridge Regression has one obvious disadvantage: unlike best subset, forward/bakward stepwise selection, which will generally select models that involve just a subset of the variables, ridge regression will include all p predictors in the final model. The penalty ∑ßj^2 will shrink all coefficients towards zero but it will not set any of them to exactly zero. This may not be a problem for prediction accuracy but it can create a challenge in model interpretation in the case where the number of variables (p) is high.\n",
    "\n",
    "The lasso is a relative recent alternative to the ridge regression that overcomes this disadvantage. The lasso coefficient ß_l minimizes the quantity:\n",
    "\n",
    "<strong>∑(y_i - ß0 - ∑ßjx_ij)^2 + lambda *∑|ßj| = RSS + lambda *∑|ßj|</strong>\n",
    "\n",
    "The lasso and ridge regression have similar formulations. The only difference is that ßj^2 term in the ridge regression penalty has been replaces by |ßj| in the lasso penalty. As with ridge regression, the lasso shrinks the coefficient estimates towards zero and the penalty has the effect of forcing some of the coefficient estimates to be exactly 0 when the tuning parameter is sufficiently large.\n",
    "\n",
    "Hence like the best subset selection, the lasso performs variable selection. As a result, models generated from the lasso are generally much easier to interpret than those produced by the ridge regression. We say that the lasso yields sparse models - models that only involve a subset of the variables.\n",
    "\n",
    "## Comparing the Lasso & Ridge Regression\n",
    "\n",
    "It is clear that the lasso has a major advantage over the ridge regression in that it produces simpler and more interpretable models that involve only a subset of the predictors. However which model leads to better prediction accuracy? Neither of the two will universally dominate the other. So a method such as cross-validation can be used to determine which approach is better.\n",
    "\n",
    "## Selecting the Tuning Parameter\n",
    "\n",
    "Cross-validation provides a simple way to tackle this problem. We choose a grid of lambda values and compute the cross validation error for each value of lambda. We then select the tuning parameter value for which the cross validation error is the smallest. Finally the model is refit using all the available observations and the selected value of the tuning parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis (PCA)\n",
    "The first principal component is the direction where observations vary the most. We want to capture as much information as we can in one single direction. Which single direction captures as much information as possible? The direction where the variance is highest amongst the projected points.\n",
    "\n",
    "The first principal component also minimizes the sum of squared perpendicular distances between point and line. Each transformed first principal component can be thought as single number summaries of all that particular observation.\n",
    "\n",
    "The second principal component must be uncorrelated to the first which makes it orthogonal (90 degrees in two dimensions) to the first. The second PC will capture less information (less spread). Plotting each PC against each variable can show how much information is captured by each one.\n",
    "\n",
    "# Principal Component Regression\n",
    "First find first M principal components where M < p then fit with least squares. Choose M with cross validation. Usually, data is standardized by standard deviation first.\n",
    "\n",
    "# Partial Least Squares\n",
    "The response does not determine the principal components. This means PCA is used in an unsupervised way. PLS is a supervised alternative to PCR. PLS generates new features as a linear combination of the old features and the response\n",
    "\n",
    "Computed by doing simple linear regression of Y onto each predictor and setting that coefficient to the linear combination coefficient for transformed variable Z1. So weights are higher for those variables with stronger relationships to response. Z2 is computed by regressing all variables against the residuals of Z1 being fit to the model. Do this iteratively (fit remaining residuals) to come up with M PLS components. Then do least squares fit on all M new dimensions. In practice PLS does not do better than PCR or ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
