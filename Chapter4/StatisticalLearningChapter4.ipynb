{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "\n",
    "The Linear regression model assumes that the response variable Y is quantitative. But in many situations the response is qualitative instead. The process of predicting the qualitative responses is called classification. Often in classification, the methods first predict the probability of each of the categories of a qualitative variable, as the basis for making the classification.\n",
    "The three of the most widely used classifiers are: logistic regression, linear discriminant analysis and K nearest neighbours.\n",
    "\n",
    "Why is linear regression not appropriate in the case of a qualititative response? This is because in general there is no natural way to convert a qualitative response variable with more than two levels (classes) into a quantitative response that is optimal/ready for linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Logistic regression models the probability that y belongs to a particular category\n",
    "Let us have a look at its model.\n",
    "\n",
    "## Logistic Model\n",
    "\n",
    "If we use the model <strong> P(X) = beta_0 + beta_1 * X </strong>, we would get values which are less than 0 and more than 1. These predictions theoretically don't make sense since the true probability must fall between 0 and 1. (Another reason, why linear regression might not be a good idea in some cases wheren qualitative responses are in play) So to avoid this problem, we must model P(x) using a function that always gives the output between 0 and 1 for all values of X. One of such functions is the logistic function:\n",
    "\n",
    "<strong> Y = 1 / (1 + e^(-t)) where t = beta_0 + beta_1 * X </strong>\n",
    "\n",
    "Some more algebra can show that :  <strong> log( p(x)/ ( 1 - p(X)) = beta_0 + beta_1 * X  </strong>\n",
    "\n",
    "The expression P(X) / (1 - P(X)) is known as <strong> odds </strong>. What logistic regression is saying is that the log odds are modelled by a linear model which can be solved by linear regression. There is no linear relationship between the probability of being in a certain class and X is logistic regression because of the need to have the probabilities between 0 and 1.\n",
    "\n",
    "The Logistic function will always produce an S shaped curve and so regardless of the value of X, we will obtain sensible information.\n",
    "\n",
    "## Estimating the Regression Coefficients\n",
    "\n",
    "In linear rregression, the model coeffiecients were found by minimizing the squared residuals.<strong> Beta_0 and Beta_1 </strong> must be estimated based on training data. Although least squared method could be used, in logistic regression, in order to estimate the coefficients, the more general method of maximum likelihood is preffered since it has better statistical properties. maximum Likelihood multiplies the model probability for each observation together and chooses parameters that maximize the number. The basic intuition behind it is as follows:\n",
    "\n",
    "we seek estimates for <strong> Beta_0 and Beta_1 </strong> such that the predicted probability p_hat(x_i) for each individual corresponds as closely as possible to the individual's record. This intuition can be formalized into a mathematical equation called a likehlihood function.\n",
    "\n",
    "<strong> L(ß_0, ß_1) = π p(x_i) π (1 - p(x_i)) </strong>\n",
    "\n",
    "The estimates ß_hat_0 and ß_hat_1 are chosen to maximize this likelihood function. The least squares method infact is a special case of maximum likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default</th>\n",
       "      <th>student</th>\n",
       "      <th>balance</th>\n",
       "      <th>income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>729.526495</td>\n",
       "      <td>44361.62507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>817.180407</td>\n",
       "      <td>12106.13470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1073.549164</td>\n",
       "      <td>31767.13895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>529.250605</td>\n",
       "      <td>35704.49394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>785.655883</td>\n",
       "      <td>38463.49588</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  default student      balance       income\n",
       "0      No      No   729.526495  44361.62507\n",
       "1      No     Yes   817.180407  12106.13470\n",
       "2      No      No  1073.549164  31767.13895\n",
       "3      No      No   529.250605  35704.49394\n",
       "4      No      No   785.655883  38463.49588"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default = pd.read_csv(\"Default.csv\")\n",
    "default.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.079823\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Yes</td>       <th>  No. Observations:  </th>   <td> 10000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  9998</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 15 Sep 2019</td> <th>  Pseudo R-squ.:     </th>   <td>0.4534</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:17:44</td>     <th>  Log-Likelihood:    </th>  <td> -798.23</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -1460.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>6.233e-290</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>  -10.6513</td> <td>    0.361</td> <td>  -29.491</td> <td> 0.000</td> <td>  -11.359</td> <td>   -9.943</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>   <td>    0.0055</td> <td>    0.000</td> <td>   24.952</td> <td> 0.000</td> <td>    0.005</td> <td>    0.006</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.13 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                    Yes   No. Observations:                10000\n",
       "Model:                          Logit   Df Residuals:                     9998\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Sun, 15 Sep 2019   Pseudo R-squ.:                  0.4534\n",
       "Time:                        13:17:44   Log-Likelihood:                -798.23\n",
       "converged:                       True   LL-Null:                       -1460.3\n",
       "Covariance Type:            nonrobust   LLR p-value:                6.233e-290\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept    -10.6513      0.361    -29.491      0.000     -11.359      -9.943\n",
       "balance        0.0055      0.000     24.952      0.000       0.005       0.006\n",
       "==============================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.13 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default['Yes'] = (default['default'] == 'Yes').astype(int)\n",
    "results = smf.logit('Yes ~ balance', data=default).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x1a26fb3400>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Looks quite a bit different than the linear regression model\n",
    "sns.lmplot('balance', 'Yes', data=default, logistic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions\n",
    "Once the Coefficients have been estimated, it is easier to calculate the probability of the response variable. Once can use qualitative predictors with the logistic regression model using the dummy variable approach.\n",
    "\n",
    "\n",
    "## Multiple Logistic Regression\n",
    "\n",
    "It can be generated as <strong> log( P(x) / (1 -P(x) ) = ß_0 + ß_1 * X_1 ----- + ß_p * X _p </strong>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.078584\n",
      "         Iterations 10\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Yes</td>       <th>  No. Observations:  </th>   <td> 10000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  9997</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>     2</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 15 Sep 2019</td> <th>  Pseudo R-squ.:     </th>   <td>0.4619</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:26:51</td>     <th>  Log-Likelihood:    </th>  <td> -785.84</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -1460.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.189e-293</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>      <td>  -10.7495</td> <td>    0.369</td> <td>  -29.115</td> <td> 0.000</td> <td>  -11.473</td> <td>  -10.026</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>student[T.Yes]</th> <td>   -0.7149</td> <td>    0.148</td> <td>   -4.846</td> <td> 0.000</td> <td>   -1.004</td> <td>   -0.426</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>balance</th>        <td>    0.0057</td> <td>    0.000</td> <td>   24.748</td> <td> 0.000</td> <td>    0.005</td> <td>    0.006</td>\n",
       "</tr>\n",
       "</table><br/><br/>Possibly complete quasi-separation: A fraction 0.15 of observations can be<br/>perfectly predicted. This might indicate that there is complete<br/>quasi-separation. In this case some parameters will not be identified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                    Yes   No. Observations:                10000\n",
       "Model:                          Logit   Df Residuals:                     9997\n",
       "Method:                           MLE   Df Model:                            2\n",
       "Date:                Sun, 15 Sep 2019   Pseudo R-squ.:                  0.4619\n",
       "Time:                        13:26:51   Log-Likelihood:                -785.84\n",
       "converged:                       True   LL-Null:                       -1460.3\n",
       "Covariance Type:            nonrobust   LLR p-value:                1.189e-293\n",
       "==================================================================================\n",
       "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "Intercept        -10.7495      0.369    -29.115      0.000     -11.473     -10.026\n",
       "student[T.Yes]    -0.7149      0.148     -4.846      0.000      -1.004      -0.426\n",
       "balance            0.0057      0.000     24.748      0.000       0.005       0.006\n",
       "==================================================================================\n",
       "\n",
       "Possibly complete quasi-separation: A fraction 0.15 of observations can be\n",
       "perfectly predicted. This might indicate that there is complete\n",
       "quasi-separation. In this case some parameters will not be identified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = smf.logit('Yes ~ balance + student', data=default).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simpsons paradox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.145434\n",
      "         Iterations 7\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>Yes</td>       <th>  No. Observations:  </th>  <td> 10000</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>  <td>  9998</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>  <td>     1</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 15 Sep 2019</td> <th>  Pseudo R-squ.:     </th> <td>0.004097</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>13:27:38</td>     <th>  Log-Likelihood:    </th> <td> -1454.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th> <td> -1460.3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>0.0005416</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "         <td></td>           <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>      <td>   -3.5041</td> <td>    0.071</td> <td>  -49.554</td> <td> 0.000</td> <td>   -3.643</td> <td>   -3.366</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>student[T.Yes]</th> <td>    0.4049</td> <td>    0.115</td> <td>    3.520</td> <td> 0.000</td> <td>    0.179</td> <td>    0.630</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                    Yes   No. Observations:                10000\n",
       "Model:                          Logit   Df Residuals:                     9998\n",
       "Method:                           MLE   Df Model:                            1\n",
       "Date:                Sun, 15 Sep 2019   Pseudo R-squ.:                0.004097\n",
       "Time:                        13:27:38   Log-Likelihood:                -1454.3\n",
       "converged:                       True   LL-Null:                       -1460.3\n",
       "Covariance Type:            nonrobust   LLR p-value:                 0.0005416\n",
       "==================================================================================\n",
       "                     coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------\n",
       "Intercept         -3.5041      0.071    -49.554      0.000      -3.643      -3.366\n",
       "student[T.Yes]     0.4049      0.115      3.520      0.000       0.179       0.630\n",
       "==================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = smf.logit('Yes ~ student', data=default).fit()\n",
    "results.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model (the multiple logistic regression model) shows a negative relationship between the student and the response (Negative Coefficient). This means that being a student decreases the likelihood of defaulting. The second model shows the exact opposite by stating that being a student increases the chances of defaulting. \n",
    "This can be explained by the fact that students have more debt on average but compared to those with the same amount of debt, they are less likely to default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiclass Logistic Regression\n",
    "The book does not cover the logistic regression with more than 2 classes. \n",
    "Mental Note: DO it on your own.\n",
    "\n",
    "### One vs All\n",
    "A simple method when you have k classes where k > 2 is to create k-1 independent logistic regression classifiers by choosing the response variable to be binary, 1 when in the current class else 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Discriminant Analysis\n",
    "\n",
    "In this alternative approach, we model the distribution of the predictors X seperately in each of the response classes and then Use Baye's theorem to flip these around into estimates for P(Y = k | X = x). When these distributions are assumed to be normal, it turns out that the model is very similar in form to logistic regresion.\n",
    "\n",
    "One might ask: Why is this method needed?:\n",
    "\n",
    "1. When classes are well seperated, the parameter estimates for the logistic regression model are surprisingly unstable. This is not the case with LDA.\n",
    "\n",
    "2. If n is small and the distribution of the predictors X is approximately normal in each of the classes, the LDA model is again more stable.\n",
    "\n",
    "## Baye's Theorem For Classification\n",
    "\n",
    "We wish to classify an observation into one probability that a randomly chosen observation comes from the kth class. Let F_k(x) = p(X = x | Y = k) denote the density function of X for an observation that comes from a kth class. \n",
    "Baye's theorem states that P(Y = k| X =x) = π_k * f_k / (∑π_l * f_l(x)).\n",
    "In general, estimating π_k is very easy if we have a random sample of Y's from the population.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (p =1)\n",
    "\n",
    "In order to estimate f_k(X) we will first make a few assumtptions about its form. Suppose that we assume f_k(x) is normal or gaussian. In the one dimensional setting, the normal density takes the form\n",
    "\n",
    "<strong> f_k(x) = 1 / (√(2π *sigma_k) * exp(1 / (sigma_k^2) * (x - µ_k)^ 2) </strong>\n",
    "\n",
    "where µ_k and sigma_k^2 are the mean and variance parameters of the kth class. For now let us further assume that there is shared variance between all the classes. Hence we find that\n",
    "\n",
    "\n",
    "<strong> P(Y = k | X = x) = π_k * 1 / (√(2π *sigma_k) * exp(1 / (sigma_k^2) * (x - µ_k)^ 2) / (∑π_if_i(x)) </strong>\n",
    "\n",
    "The Baye's classifier involves assigning an observation X = x to the class for which the above expression's value is the largest.\n",
    "\n",
    "In practise, even if we are certain of our assumption that X is drawn from a guassian distribution within each class, we still have to estimate the parameters µ_1, µ_2, µ_3... and π_1,π_2,π_3... and sigma^2. The LDA approximates the Baye's Classifier by plugging the estimates for π_k, µ_k and sigma^2:\n",
    "\n",
    "<strong>\n",
    "µ_k = ∑x/n_k\n",
    "\n",
    "sigma = ∑∑(x_i - µ_k)^2/(n - k)\n",
    "\n",
    "π_k = n_k / n\n",
    "</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA (p>1)\n",
    "\n",
    "We now extend the LDA classifier to the case of multiple predictors. To do this we will assume that X is drawn from a multivariate Gaussian Distribution with a class specific mean vector and a common covariance matrix.\n",
    "\n",
    "<strong> Multivariate Gaussian Distribution: Assumes each predictor follows a one-dimensional normal distribution with some coprrelation between each pair of predictors. To indicate that a p dimensional random variable X has multivariate Gaussian distribution, we write X ˜ N(µ, ∑) where E(X) = µ.\n",
    "\n",
    "f(x) = 1/(2π^(p/2) * |∑|^1/2) * exp(1/2 * (x - µ)^T * ∑ ^-1 * (x - µ))\n",
    "\n",
    "In the cas eof p > 1 predictors, the lDA classifier assumes that the observations in the kth class are drawn from a multivariate Gaussian distribution.\n",
    "</strong>\n",
    "\n",
    "Plugging the density function for the Kth class into the equation P(Y =k| X=x) and performing a little bit of algebra, reveals that the Bayes Classifer assigns an observation X = x to the class for which\n",
    "\n",
    "expression = X.T * ∑ ^ -1 *µ_k - 1/2 µ_k.T * ∑ ^-1 *µ_k + log(π_k) is the largest\n",
    "\n",
    "As the threshold is reduced, the error rate among individuals who default decreases but the error rate among the individuals who do not default increases. How can one decide which threshold value is the best? Such a decision must be made based obn domain Knowledge.\n",
    "\n",
    "The ROC curve is a popular graphic for simultaneoulsy displaying the two types of errors for all the possible thresholds. The overall performance and of a classifier, summarized over all possible thresholds is given by the area under the ROC curve (AOC). An ideal ROC curve will hug the top left corner so the larger the AUC, the better the classifer. ROC curves are useful for comparing the different classifiers since they take into account all the possible threshholds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quadratic Discriminant Analysis\n",
    "\n",
    "LDA assumes that the observations within each class are drawn from a multivariate gaussian distribution with a class specific mean vector and a covariance matrix that is common to all the classes. QDA assumes that observations are drawn from a Gaussian Distribution but each class has its own covariance matrix. under this assumption, The Baye's classifer assigns an observation X = x to the class for which\n",
    "\n",
    "expression = - 1/2 (x - µ_k).T * ∑ ^-1 *(x - µ_k) - 1/2 log|∑_k| + log(π_k)\n",
    "is the largest.\n",
    "\n",
    "This might one beg the question that why does it matter if each class has its own covariance matrix? The answer lies in the bias-variance tradeoff.\n",
    "\n",
    "LDA is much less flexible than QDA and thus has low variance. This can potentially lead to improved prediction performance. but there is a tradeoff. If LDA's assumption that the K classes share a common covariance matrix is badly off, then the LDA can suffer from high bias. Roughly speaking, the LDA tends to perform better than QDA if there are relatively low training observations and so reducing the varianc eis crucial. In contrast, QDA is suggested when the training data is very large so that the variance of the classifier is not a major concern or if the assumption of a common covariance matrix for the K classes is clearly untenable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparision Of Classification Methods\n",
    "\n",
    "Though their motivations differ, the logistic regression and the LDA methods are closely connected. Both logistic regression and the LDA produce linear decision boundaries. The only difference is that ß_0 and ß_1 are estimated using maximum likelihood whereas c_0 and c_1 are calculated using the estimated mean and variance from a normal distribution. KNN is a non parametric approach. No assumptions are made about the shape of the decision boundary. Therefore we can expect this method to dominate the LDA and logistic regression when the decision boundary is highly non-linear. however, KNN doesnt tell us which predictions are important. this is where QDA comes into play. It serves as a compromise between the non paramtric KNN method and the LDA and the logistic regression method. Since QDA assumes a quadratic decision boundary, it can accurately model a wide range of problems as compared to the linear methods.\n",
    "\n",
    "<strong> when true decision boundaries are linear then the LDA and logistic regression approaches will perform well. When boundaries are moderately non-linear QDA will give better results. Finally for more comp0licated decision boundaries, a non parametric method such as KNN can be superior.</strong>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
