{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods\n",
    "\n",
    "Resampling methods are an indispensable tool in the world of modern statistics. They involve repeatedly drawing samples from a training set and refitting a model of an interest on each sample in order to obtain additional information about the fitted information. For example, in order to estimate the variability of a linear regression fit, we can repeatdely draw different samples from the training data, fit a linear regression to each new sample and then examine the extent to which the resulting fits differ. Such an approach allows us to obtain information that would not be available from fitting the data only once using the original training sample.\n",
    "\n",
    "However (goes without saying), resampling approaches can be computationally expensive because they involve fitting some statistical method multiple times using different subsets of the training data. The two most common resampling methods are: <strong> Cross-Validation and Bootstrap </strong>. Both methods are important tools in the practical application of many statistical learning procedures. Ex: Cross-Validation can be used to estimate the test error associated with a given statisticial method in order to evaluate its performance or to select the appropriate level of flexibility. The process of evaluating a model's performance is known as <strong> model assessment</strong> whereas the process of selecting the proper level of flexibility for a model is known as <strong> model selection </strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation\n",
    "The test error rate can be easily calculated if a designated test set is available. In the absence of a very large designated test set that can be used to directly estimate the test error rate, a number of techniques can be used to estimate the quantity using the available training data. We are going to consider a class of methods that estimate the test error rate by holding out a subset of training observations from the fitting process and then applying the statistical learning method to those held out observations.\n",
    "\n",
    "## Validation Set Approach\n",
    "This method involves randomly dividing the available set of observations into two parts: a training set and a validation set. The model is fit on the training set and the fitted model is used to predict the responses for the observations in the validation set. The resulting validation set error rate (typically calculated via MSE in the case of a quantitative response) provides an estimate of the test error rate.\n",
    "The validation set approach is conceptually simple and easy to implement. But it has two potential drawbacks:\n",
    "1. The validation set estimate of the test errir can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set.\n",
    "2. in the validation approach, only a subset of these observations - those that are included in the training set rather than the validation set - are used to fit the model. Since statistical methods tend to perform worse when trained on fewer observations, it suggests that the validation set error may tend to overestimate the test error rate for the model fit on the entire data set.\n",
    "\n",
    "## Leave-One-Out Cross Validation\n",
    "LOOCV is a closley related method to the Validation Set Approach mentioned above. LOOCV involves splitting the set of observations into two parts. However, instead of creating two subsets of compaarable size, a single observation (x1, y1) is used for the validation set and the remaining observations make up the training set. The statistical learning method is fit on the n-1 training observations and a prediction y_hat is made for the excluded observation using its value x1. Since (x1, y1) was not used in the training process, the MSE provides an unbiased estimate for the test error. It is still, however, a poor estimate because it is highly variable since it is based on a single observation.\n",
    "We can repeat the procedure by selecting (x2, y2) for the validation data, training the model on the rest of the n - 1 observations and computing MSE2. Repeating this process n times produces n squared errors: <strong> MSE1, MSE2.....MSEn </strong>. The LOOCv estimate for the test estimate is just the average of these erros.\n",
    "\n",
    "LOOCv has a couple of advantages over the Validation Set Approach:\n",
    "1. It has far less bias. Consequently, the LOOCV approach tends not to overestimate the test error rate as much as the validation set approach.\n",
    "2. Performing LOOCV multiple times always yields the same results: there is absolutely no randomness in the training/validation set points.\n",
    "\n",
    "However, LOOCV has the potential to be expensive to implement since the training model has to be fit n times. this can be very time consuming if the n is large and if each individual item is slow to fit.\n",
    "<strong> LOOCV is a very general method and can be used with any kind of predictive modelling </strong>.\n",
    "\n",
    "## K-Fold Cross Validation\n",
    "An alternative to LOOCV is the K-fold CV. This approach involves randomly dividing the set of observations into K groups or folds of approximately equal size. The first fold is treated as the validation set and the method is fit on the remaining K - 1 folds. The MSE is then computed on the observations in the held out fold. This procedure is repeated k times, each time a different group of observations is treated as the validation set. This process results in k-estimates of the test error: <strong> MSE1,  MSE2.....MSEk </strong>. the k-fold CV estimate is computing by averaging these values.\n",
    "\n",
    "<strong> LOOCV is a special kind of k-fold cross validation </strong>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bias - Variance Trade Off\n",
    "A more important advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than LOOCV. This has to do with bias-variance tradeoff. It is not hard to see the LOOCV will give approximately unbiased estimates of the test error, since each training set contains n-1 observations, which is almost as many as the number of observations in the full data set. Performing K-fold will lead to an intermediate level of bias since each training set contains (K - 1) * n / k observations. therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred over K-fold CV.\n",
    "\n",
    "But bias is not the only source of concern in an estimating procedure, we must also consider the procedure's variance as we all know. It turns out the LOOCV has higher variance than K-fold CV with  k < n. Why you might ask> Well when we perform LOOCV, we are in effect averaging the outputs of n filled models, each of which is trained on an almost identical set of observations: therefore these outputs are highlly correlated with each other. In contrast, when we perform K-fold CV with k < n, we are averaging the outputs of k-filled models that are somewhat less correlated with each other since the overlap between the training sets in each model is smaller. Since the mean of many of the highly correlated quanitites has higher variance than the mean of many quantities that are not that highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than the test error of the K-fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
